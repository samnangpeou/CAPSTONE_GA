{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/country_lyrics.csv')\n",
    "df.dropna(inplace=True)\n",
    "df['lyrics_length'] = df['lyrics'].str.len()\n",
    "df['lyrics_word_count'] = df['lyrics'].str.split().apply(len)\n",
    "df.drop([i for i in df[df['lyrics'].str.len() < 400].index],\n",
    "        inplace=True)\n",
    "df.drop([i for i in df['lyrics_word_count'].sort_values(ascending=True)[:50].index],\n",
    "        inplace=True)\n",
    "df.drop([i for i in df[df['url'].str.contains('christmas')].index],\n",
    "       inplace=True)\n",
    "df.drop([i for i in df[df['url'].str.contains('winter')].index],\n",
    "       inplace=True)\n",
    "df.drop([i for i in df[df['url'].str.contains('snow')].index], inplace=True)\n",
    "df.drop([i for i in df[df['url'].str.contains('noel')].index],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_lyrics = \"\\n \".join([i for i in df['lyrics']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 7215750 characters\n"
     ]
    }
   ],
   "source": [
    "print('Length of text: {} characters'.format(len(country_lyrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Her day starts with a coffee and ends with a wine\n",
      "Takes forever getting ready so she's never on time for anything\n",
      "When she gets that \"come get me\" look in her eyes\n",
      "Well, it kinda scares me, the way that she drives me wild\n",
      "When she drives me wild\n",
      "Beau\n"
     ]
    }
   ],
   "source": [
    "print(country_lyrics[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(country_lyrics))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.preprocessing.string_lookup.StringLookup at 0x7fbf48895400>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_from_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[63, 64, 65, 66, 67, 68, 69], [86, 87, 88]]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7215750,), dtype=int64, numpy=array([39, 67, 80, ..., 87, 67, 81])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(country_lyrics, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "e\n",
      "r\n",
      " \n",
      "d\n",
      "a\n",
      "y\n",
      " \n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(country_lyrics)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'H' b'e' b'r' b' ' b'd' b'a' b'y' b' ' b's' b't' b'a' b'r' b't' b's'\n",
      " b' ' b'w' b'i' b't' b'h' b' ' b'a' b' ' b'c' b'o' b'f' b'f' b'e' b'e'\n",
      " b' ' b'a' b'n' b'd' b' ' b'e' b'n' b'd' b's' b' ' b'w' b'i' b't' b'h'\n",
      " b' ' b'a' b' ' b'w' b'i' b'n' b'e' b'\\n' b'T' b'a' b'k' b'e' b's' b' '\n",
      " b'f' b'o' b'r' b'e' b'v' b'e' b'r' b' ' b'g' b'e' b't' b't' b'i' b'n'\n",
      " b'g' b' ' b'r' b'e' b'a' b'd' b'y' b' ' b's' b'o' b' ' b's' b'h' b'e'\n",
      " b\"'\" b's' b' ' b'n' b'e' b'v' b'e' b'r' b' ' b'o' b'n' b' ' b't' b'i'\n",
      " b'm' b'e' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Her day starts with a coffee and ends with a wine\\nTakes forever getting ready so she's never on time \"\n",
      "b'for anything\\nWhen she gets that \"come get me\" look in her eyes\\nWell, it kinda scares me, the way that'\n",
      "b\" she drives me wild\\nWhen she drives me wild\\nBeautiful, crazy, she can't help but amaze me\\nThe way tha\"\n",
      "b\"t she dances, ain't afraid to take chances\\nAnd wears her heart on her sleeve\\nYeah, she's crazy but he\"\n",
      "b\"r crazy's beautiful to me\\nShe makes plans for the weekend, can't wait to go out\\nTill she changes her \"\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b\"Her day starts with a coffee and ends with a wine\\nTakes forever getting ready so she's never on time\"\n",
      "Target: b\"er day starts with a coffee and ends with a wine\\nTakes forever getting ready so she's never on time \"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True, \n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else: \n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_from_chars.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30*25/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 135) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  34560     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  138375    \n",
      "=================================================================\n",
      "Total params: 4,111,239\n",
      "Trainable params: 4,111,239\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b\"ight now\\nYeah, I learned how to let go\\nAnd how to take the high road\\nI'm on another level\\nYou can't \"\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"W3m\\xe2\\x80\\x8bO rI44\\xc3\\xaa'\\xc3\\xb3\\xc3\\xban\\xe2\\x80\\x85Bgh\\xe2\\x80\\x9dkH*x\\xc3\\xb4YQ\\xc3\\xae-?\\xc3\\xad!(L\\xc3\\xa4aa\\xe2\\x80\\x93i\\nv5-;\\xe2\\x80\\x93\\xe2\\x80\\x8b,fR\\xe2\\x80\\x8b\\xc3\\x87?\\xc2\\xa0\\xc3\\xb9H\\xc5\\x93Ah\\xe2\\x80\\xa6.\\xd0\\xb5KD iU\\xc3\\xade\\xe2\\x80\\x85ua\\xe2\\x80\\x8bZLv4}\\xc3\\x879 \\xc3\\xb9Q\\xc2\\xa3\\xc3\\xa8ULl\\xc3\\x87\\xc2\\xb4!\\xc3\\x80[UNK]\\xe2\\x80\\x98Z`h_\\xc3\\xa4\\\\\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 135)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         4.904089\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134.84001"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005), loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1116/1116 [==============================] - 1284s 1s/step - loss: 1.5692\n",
      "Epoch 2/30\n",
      "1116/1116 [==============================] - 1910s 2s/step - loss: 1.2432\n",
      "Epoch 3/30\n",
      "1116/1116 [==============================] - 2104s 2s/step - loss: 1.1571\n",
      "Epoch 4/30\n",
      "1116/1116 [==============================] - 2100s 2s/step - loss: 1.1042\n",
      "Epoch 5/30\n",
      "1116/1116 [==============================] - 2100s 2s/step - loss: 1.0604\n",
      "Epoch 6/30\n",
      "1116/1116 [==============================] - 2103s 2s/step - loss: 1.0233\n",
      "Epoch 7/30\n",
      "1116/1116 [==============================] - 2151s 2s/step - loss: 0.9890\n",
      "Epoch 8/30\n",
      "1116/1116 [==============================] - 2164s 2s/step - loss: 0.9576\n",
      "Epoch 9/30\n",
      "1116/1116 [==============================] - 2167s 2s/step - loss: 0.9291\n",
      "Epoch 10/30\n",
      "1116/1116 [==============================] - 2166s 2s/step - loss: 0.9026\n",
      "Epoch 11/30\n",
      "1116/1116 [==============================] - 2111s 2s/step - loss: 0.8790\n",
      "Epoch 12/30\n",
      "1116/1116 [==============================] - 2120s 2s/step - loss: 0.8593\n",
      "Epoch 13/30\n",
      "1116/1116 [==============================] - 1431s 1s/step - loss: 0.8406\n",
      "Epoch 14/30\n",
      "1116/1116 [==============================] - 1401s 1s/step - loss: 0.8241\n",
      "Epoch 15/30\n",
      "1116/1116 [==============================] - 1420s 1s/step - loss: 0.8122\n",
      "Epoch 16/30\n",
      "1116/1116 [==============================] - 1406s 1s/step - loss: 0.7998\n",
      "Epoch 17/30\n",
      "1116/1116 [==============================] - 1441s 1s/step - loss: 0.7916\n",
      "Epoch 18/30\n",
      "1116/1116 [==============================] - 1411s 1s/step - loss: 0.7817\n",
      "Epoch 19/30\n",
      "1116/1116 [==============================] - 1311s 1s/step - loss: 0.7747\n",
      "Epoch 20/30\n",
      "1116/1116 [==============================] - 1310s 1s/step - loss: 0.7695\n",
      "Epoch 21/30\n",
      "1116/1116 [==============================] - 1312s 1s/step - loss: 0.7647\n",
      "Epoch 22/30\n",
      "1116/1116 [==============================] - 1333s 1s/step - loss: 0.7602\n",
      "Epoch 23/30\n",
      "1116/1116 [==============================] - 1350s 1s/step - loss: 0.7564\n",
      "Epoch 24/30\n",
      "1116/1116 [==============================] - 1370s 1s/step - loss: 0.7525\n",
      "Epoch 25/30\n",
      "1116/1116 [==============================] - 1363s 1s/step - loss: 0.7504\n",
      "Epoch 26/30\n",
      "1116/1116 [==============================] - 1436s 1s/step - loss: 0.7470\n",
      "Epoch 27/30\n",
      "1116/1116 [==============================] - 1411s 1s/step - loss: 0.7459\n",
      "Epoch 28/30\n",
      "1116/1116 [==============================] - 1379s 1s/step - loss: 0.7442\n",
      "Epoch 29/30\n",
      "1116/1116 [==============================] - 1366s 1s/step - loss: 0.7429\n",
      "Epoch 30/30\n",
      "1116/1116 [==============================] - 1309s 1s/step - loss: 0.7411\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature=temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['','[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices = skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())]) \n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits] \n",
    "    predicted_logits, states =  self.model(inputs=input_ids, states=states, \n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0004"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0001 - 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOLOL Ray Month Carolina\n",
      "Says it wasn't fine\n",
      "How could it back down\n",
      "And some like to sip, square tonight\n",
      "I \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 0.14001107215881348\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['LOLOL '])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "\n",
    "print(f\"\\nRun time: {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
